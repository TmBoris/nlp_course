{"cells":[{"cell_type":"markdown","metadata":{"id":"b1acf78a"},"source":["# Глубинное обучение для текстовых данных, ФКН ВШЭ\n","\n","## Домашнее задание 2: Рекуррентные нейронные сети\n","\n","### Оценивание и штрафы\n","\n","Максимально допустимая оценка за работу — __10 (+5) баллов__. Сдавать задание после указанного срока сдачи нельзя.\n","\n","Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n","\n","Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n","\n","__Мягкий дедлайн: 14.10.24 23:59__   \n","__Жесткий дедлайн: 17.10.24 23:59__\n","\n","### О задании\n","\n","В этом задании вам предстоит самостоятельно реализовать модель LSTM для решения задачи классификации с пересекающимися классами (multi-label classification). Это вид классификации, в которой каждый объект может относиться одновременно к нескольким классам. Такая задача часто возникает при классификации фильмов по жанрам, научных или новостных статей по темам, музыкальных композиций по инструментам и так далее.\n","\n","В нашем случае мы будем работать с датасетом биотехнических новостей и классифицировать их по темам. Этот датасет уже предобработан: текст приведен к нижнему регистру, удалена пунктуация, все слова разделены проблелом."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:24.396842Z","iopub.status.busy":"2024-10-17T15:16:24.396470Z","iopub.status.idle":"2024-10-17T15:16:45.019183Z","shell.execute_reply":"2024-10-17T15:16:45.018202Z","shell.execute_reply.started":"2024-10-17T15:16:24.396807Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gdown in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from gdown) (3.15.4)\n","Requirement already satisfied: requests[socks] in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from gdown) (2.29.0)\n","Requirement already satisfied: tqdm in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.19)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from requests[socks]->gdown) (2024.6.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/bspanfilov/.pyenv/versions/3.9.19/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n","Note: you may need to restart the kernel to use updated packages.\n","zsh:1: no matches found: https://drive.google.com/uc?id=1OCbRPznUPXmj9IC410HzL4VldgUhXZCm\n"]}],"source":["%pip install gdown\n","!gdown https://drive.google.com/uc?id=1OCbRPznUPXmj9IC410HzL4VldgUhXZCm"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.021535Z","iopub.status.busy":"2024-10-17T15:16:45.021170Z","iopub.status.idle":"2024-10-17T15:16:45.083397Z","shell.execute_reply":"2024-10-17T15:16:45.082664Z","shell.execute_reply.started":"2024-10-17T15:16:45.021500Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import re\n","import random\n","import os\n","from nltk.corpus import stopwords\n","import torch\n","import wandb\n","import warnings\n","from collections import defaultdict\n","warnings.filterwarnings(\"ignore\")\n","\n","wandb.login(key='46c3b8e339b3fb22dc286204510c8af5b2c3e2e5')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.084647Z","iopub.status.busy":"2024-10-17T15:16:45.084355Z","iopub.status.idle":"2024-10-17T15:16:45.091723Z","shell.execute_reply":"2024-10-17T15:16:45.090871Z","shell.execute_reply.started":"2024-10-17T15:16:45.084616Z"},"trusted":true},"outputs":[],"source":["def set_random_seed(seed):\n","    \"\"\"\n","    Set random seed for model training or inference.\n","\n","    Args:\n","        seed (int): defines which seed to use.\n","    \"\"\"\n","    # fix random seeds for reproducibility\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    # benchmark=True works faster but reproducibility decreases\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","set_random_seed(1)"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.execute_input":"2024-10-17T15:16:45.094096Z","iopub.status.busy":"2024-10-17T15:16:45.093802Z","iopub.status.idle":"2024-10-17T15:16:45.210310Z","shell.execute_reply":"2024-10-17T15:16:45.209284Z","shell.execute_reply.started":"2024-10-17T15:16:45.094065Z"},"id":"af1a5fff","outputId":"891c58cd-6964-4319-ade3-92bb90356f93","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drive your plow over the bones of the dead by ...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in the recently tabled national budget denel h...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>shares take a break its good for you picture g...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>reso is currently hiring for two positions pro...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>charter buyer club what is the charter buyer c...</td>\n","      <td>other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text labels\n","0  drive your plow over the bones of the dead by ...  other\n","1  in the recently tabled national budget denel h...  other\n","2  shares take a break its good for you picture g...  other\n","3  reso is currently hiring for two positions pro...  other\n","4  charter buyer club what is the charter buyer c...  other"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["dataset = pd.read_csv('biotech_news.tsv', sep='\\t')\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"HRBZwYd9QMMS"},"source":["## Предобработка лейблов\n","\n","\n","__Задание 1 (1.5 балла)__. Как вы можете заметить, лейблы записаны в виде строк, разделенных запятыми. Для работы с ними нам нужно преобразовать их в числа. Так как каждый объект может принадлежать нескольким классам, закодируйте лейблы в виде векторов из 0 и 1, где 1 означает, что объект принадлежит соответствующему классу, а 0 – не принадлежит. Имея такую кодировку, мы сможем обучить модель, решая задачу бинарной классификации для каждого класса."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.212125Z","iopub.status.busy":"2024-10-17T15:16:45.211705Z","iopub.status.idle":"2024-10-17T15:16:45.222337Z","shell.execute_reply":"2024-10-17T15:16:45.221345Z","shell.execute_reply.started":"2024-10-17T15:16:45.212077Z"},"trusted":true},"outputs":[],"source":["labels = set()\n","for label_lst in dataset.labels.unique():\n","    for label in label_lst.split(', '):\n","        labels.add(label)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.223970Z","iopub.status.busy":"2024-10-17T15:16:45.223589Z","iopub.status.idle":"2024-10-17T15:16:45.230472Z","shell.execute_reply":"2024-10-17T15:16:45.229603Z","shell.execute_reply.started":"2024-10-17T15:16:45.223925Z"},"trusted":true},"outputs":[],"source":["label_to_num = {v:k for k, v in enumerate(labels)}"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.232032Z","iopub.status.busy":"2024-10-17T15:16:45.231668Z","iopub.status.idle":"2024-10-17T15:16:45.241036Z","shell.execute_reply":"2024-10-17T15:16:45.240058Z","shell.execute_reply.started":"2024-10-17T15:16:45.231990Z"},"trusted":true},"outputs":[],"source":["def get_labels_array(labels: str):\n","    target = np.zeros(len(label_to_num))\n","    for label in labels.split(', '):\n","        target[label_to_num[label]] = 1\n","\n","    return target"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.242589Z","iopub.status.busy":"2024-10-17T15:16:45.242219Z","iopub.status.idle":"2024-10-17T15:16:45.258729Z","shell.execute_reply":"2024-10-17T15:16:45.258041Z","shell.execute_reply.started":"2024-10-17T15:16:45.242548Z"},"trusted":true},"outputs":[],"source":["dataset['num_labels'] = dataset['labels'].apply(get_labels_array)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.259911Z","iopub.status.busy":"2024-10-17T15:16:45.259665Z","iopub.status.idle":"2024-10-17T15:16:45.276607Z","shell.execute_reply":"2024-10-17T15:16:45.275694Z","shell.execute_reply.started":"2024-10-17T15:16:45.259883Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>num_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drive your plow over the bones of the dead by ...</td>\n","      <td>other</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in the recently tabled national budget denel h...</td>\n","      <td>other</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>shares take a break its good for you picture g...</td>\n","      <td>other</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text labels  \\\n","0  drive your plow over the bones of the dead by ...  other   \n","1  in the recently tabled national budget denel h...  other   \n","2  shares take a break its good for you picture g...  other   \n","\n","                                          num_labels  \n","0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Предобработка данных"]},{"cell_type":"markdown","metadata":{"id":"vMe0c5AAXM8d"},"source":["В этом задании мы будем обучать рекуррентные нейронные сети. Как вы знаете, они работают лучше для коротких текстов, так как не очень хорошо улавливают далекие зависимости. Для уменьшение длин текстов их стоит почистить.\n","\n","Сразу разделим выборку на обучающую и тестовую, чтобы считать все нужные статистики только по обучающей."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.280309Z","iopub.status.busy":"2024-10-17T15:16:45.280012Z","iopub.status.idle":"2024-10-17T15:16:45.289621Z","shell.execute_reply":"2024-10-17T15:16:45.288776Z","shell.execute_reply.started":"2024-10-17T15:16:45.280279Z"},"id":"f8135000","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","texts_train, texts_test, y_train, y_test = train_test_split(\n","    dataset['text'].to_numpy(),\n","    dataset['num_labels'].to_numpy(),\n","    test_size=0.2,  # do not change this\n","    random_state=0  # do not change this\n",")"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.290971Z","iopub.status.busy":"2024-10-17T15:16:45.290678Z","iopub.status.idle":"2024-10-17T15:16:45.340647Z","shell.execute_reply":"2024-10-17T15:16:45.339782Z","shell.execute_reply.started":"2024-10-17T15:16:45.290940Z"},"trusted":true},"outputs":[],"source":["new_y_train = np.zeros((len(y_train), len(y_train[0])))\n","\n","for i in range(len(y_train)):\n","    for j in range(len(y_train[0])):\n","        new_y_train[i, j] = y_train[i][j]\n","\n","y_train = new_y_train"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.341903Z","iopub.status.busy":"2024-10-17T15:16:45.341662Z","iopub.status.idle":"2024-10-17T15:16:45.357957Z","shell.execute_reply":"2024-10-17T15:16:45.357106Z","shell.execute_reply.started":"2024-10-17T15:16:45.341876Z"},"trusted":true},"outputs":[],"source":["new_y_test = np.zeros((len(y_test), len(y_test[0])))\n","\n","for i in range(len(y_test)):\n","    for j in range(len(y_test[0])):\n","        new_y_test[i, j] = y_test[i][j]\n","\n","y_test = new_y_test"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.359608Z","iopub.status.busy":"2024-10-17T15:16:45.359132Z","iopub.status.idle":"2024-10-17T15:16:45.372217Z","shell.execute_reply":"2024-10-17T15:16:45.371420Z","shell.execute_reply.started":"2024-10-17T15:16:45.359557Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2431,), (2431, 29), (608,), (608, 29))"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["texts_train.shape, y_train.shape, texts_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["__Задание 2 (1.5 балла)__. Удалите из текстов стоп слова, слишком редкие и слишком частые слова. Гиперпараметры подберите самостоятельно (в идеале их стоит подбирать по качеству на тестовой выборке). Если вы считаете, что стоит добавить еще какую-то обработку, то сделайте это. Важно не удалить ничего, что может повлиять на предсказание класса."]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.373440Z","iopub.status.busy":"2024-10-17T15:16:45.373161Z","iopub.status.idle":"2024-10-17T15:16:45.388504Z","shell.execute_reply":"2024-10-17T15:16:45.387734Z","shell.execute_reply.started":"2024-10-17T15:16:45.373408Z"},"id":"BcmyCcoaXIqy","trusted":true},"outputs":[],"source":["stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":52,"id":"1c395f1a","metadata":{},"outputs":[],"source":["word_to_num = defaultdict(int)\n","for text in texts_train:\n","    text = text.split()\n","    for word in text:\n","        word_to_num[word] += 1"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.390102Z","iopub.status.busy":"2024-10-17T15:16:45.389760Z","iopub.status.idle":"2024-10-17T15:16:45.394605Z","shell.execute_reply":"2024-10-17T15:16:45.393711Z","shell.execute_reply.started":"2024-10-17T15:16:45.390068Z"},"trusted":true},"outputs":[],"source":["def remove_stopwords(text):\n","    clear_texts = []\n","    clear_text = [word for word in text.split(' ') if word not in stop_words and 10 < word_to_num[word] < 2500]\n","\n","    return ' '.join(clear_text)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.396146Z","iopub.status.busy":"2024-10-17T15:16:45.395800Z","iopub.status.idle":"2024-10-17T15:16:48.479473Z","shell.execute_reply":"2024-10-17T15:16:48.478423Z","shell.execute_reply.started":"2024-10-17T15:16:45.396104Z"},"trusted":true},"outputs":[],"source":["texts_train = np.array([remove_stopwords(text) for text in texts_train])\n","texts_test = np.array([remove_stopwords(text) for text in texts_test])"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:48.481409Z","iopub.status.busy":"2024-10-17T15:16:48.480997Z","iopub.status.idle":"2024-10-17T15:16:48.489496Z","shell.execute_reply":"2024-10-17T15:16:48.488626Z","shell.execute_reply.started":"2024-10-17T15:16:48.481339Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2431,), (2431, 29), (608,), (608, 29))"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["texts_train.shape, y_train.shape, texts_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["__Задание 3 (2 балла)__. Осталось перевести тексты в индексы токенов, чтобы их можно было подавать в модель. У вас есть две опции, как это сделать:\n","1. __(+0 баллов)__ Токенизировать тексты по словам.\n","2. __(до +5 баллов)__ Реализовать свою токенизацию BPE. Количество баллов будет варьироваться в зависимости от эффективности реализации. При реализации нельзя пользоваться специализированными библиотеками.\n","\n","Токенизируйте тексты, переведите их в списки индексов и сложите вместе с лейблами в `DataLoader`. Не забудьте добавить в `DataLoader` `collate_fn`, которая будет дополнять все короткие тексты в батче паддингами. Для маппинга токенов в индексы вам может пригодиться `gensim.corpora.dictionary.Dictionary`."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:48.490860Z","iopub.status.busy":"2024-10-17T15:16:48.490589Z","iopub.status.idle":"2024-10-17T15:16:48.503229Z","shell.execute_reply":"2024-10-17T15:16:48.502410Z","shell.execute_reply.started":"2024-10-17T15:16:48.490831Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import time\n","from IPython.display import clear_output\n","\n","\n","def word_tokenizer(texts: list[str]) -> list[list]:\n","    return [text.split(' ') for text in texts]\n","\n","def get_stats(texts):\n","    pairs = defaultdict(int)\n","    for text in texts:\n","        for i in range(len(text) - 1):\n","            pairs[(text[i], text[i + 1])] += 1\n","    return pairs\n","\n","def merge_vocab(pair, texts):\n","    first, second = pair\n","    new_symbol = ''.join([first, second])\n","    new_texts = []\n","    for text in texts:\n","        new_text = []\n","        i = 0\n","        while i < len(text) - 1:\n","            if text[i] == first and text[i + 1] == second:\n","                new_text.append(new_symbol)\n","                i += 1\n","            else:\n","                new_text.append(text[i])\n","            i += 1\n","        new_texts.append(new_text)\n","    \n","    return new_texts\n","\n","def bpe_tokenizer(texts: list[str], num_merges):\n","    texts = [list(x) for x in texts]\n","    for i in range(num_merges):\n","        clear_output()\n","        print('=================')\n","        print(f'Starting loop {i}')\n","        start = time.time()\n","\n","        pairs = get_stats(texts)\n","\n","        print(f'time for get_stats:', time.time() - start)\n","        start = time.time()\n","\n","        best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n","\n","        print(f'time for get best_pair:', time.time() - start)\n","        start = time.time()\n","\n","        texts = merge_vocab(best_pair, texts)\n","        \n","        print(f'time for merge vocabs:', time.time() - start)\n","    return texts\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:48.504547Z","iopub.status.busy":"2024-10-17T15:16:48.504265Z","iopub.status.idle":"2024-10-17T15:16:49.645801Z","shell.execute_reply":"2024-10-17T15:16:49.644828Z","shell.execute_reply.started":"2024-10-17T15:16:48.504517Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["from tokenizers import Tokenizer, models, pre_tokenizers, processors, trainers\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.models import BPE\n","\n","# 1. Создаём пустой BPE токенизатор\n","tokenizer = Tokenizer(BPE())\n","\n","# 2. Используем пробел как метод разбиения на слова\n","tokenizer.pre_tokenizer = Whitespace()\n","\n","# 3. Создаём обучающий тренер для BPE\n","trainer = BpeTrainer(special_tokens=[\" \"], vocab_size=16000)\n","\n","# 4. Соберём примеры предложений для обучения токенизатора\n","texts = texts_train\n","\n","# 5. Обучаем BPE токенизатор\n","tokenizer.train_from_iterator(texts, trainer)\n","\n","# 6. Сохраняем токенизатор на диск (по желанию)\n","# tokenizer.get_vocab()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:49.647323Z","iopub.status.busy":"2024-10-17T15:16:49.647005Z","iopub.status.idle":"2024-10-17T15:16:50.676738Z","shell.execute_reply":"2024-10-17T15:16:50.675913Z","shell.execute_reply.started":"2024-10-17T15:16:49.647289Z"},"trusted":true},"outputs":[],"source":["tokenized_train = tokenizer.encode_batch_fast(texts_train)\n","tokenized_test = tokenizer.encode_batch_fast(texts_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["### секретик"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.678790Z","iopub.status.busy":"2024-10-17T15:16:50.678049Z","iopub.status.idle":"2024-10-17T15:16:50.683046Z","shell.execute_reply":"2024-10-17T15:16:50.682138Z","shell.execute_reply.started":"2024-10-17T15:16:50.678744Z"},"trusted":true},"outputs":[],"source":["# import gensim\n","# from gensim.corpora.dictionary import Dictionary\n","\n","# dictionary = Dictionary(word_tokenizer(np.concatenate((texts_train, texts_test))))\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.684825Z","iopub.status.busy":"2024-10-17T15:16:50.684276Z","iopub.status.idle":"2024-10-17T15:16:50.692135Z","shell.execute_reply":"2024-10-17T15:16:50.691404Z","shell.execute_reply.started":"2024-10-17T15:16:50.684790Z"},"trusted":true},"outputs":[],"source":["# len(texts_train), list(texts_train[0])[:5]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.693418Z","iopub.status.busy":"2024-10-17T15:16:50.693115Z","iopub.status.idle":"2024-10-17T15:16:50.701698Z","shell.execute_reply":"2024-10-17T15:16:50.700871Z","shell.execute_reply.started":"2024-10-17T15:16:50.693361Z"},"trusted":true},"outputs":[],"source":["# tokenized = bpe_tokenizer(texts_train, 300)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.702955Z","iopub.status.busy":"2024-10-17T15:16:50.702658Z","iopub.status.idle":"2024-10-17T15:16:50.710733Z","shell.execute_reply":"2024-10-17T15:16:50.709871Z","shell.execute_reply.started":"2024-10-17T15:16:50.702911Z"},"trusted":true},"outputs":[],"source":["# dictionary = Dictionary(tokenized)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.712119Z","iopub.status.busy":"2024-10-17T15:16:50.711778Z","iopub.status.idle":"2024-10-17T15:16:50.721110Z","shell.execute_reply":"2024-10-17T15:16:50.720354Z","shell.execute_reply.started":"2024-10-17T15:16:50.712069Z"},"trusted":true},"outputs":[],"source":["# from torch.utils.data import Dataset\n","\n","# class BaseDataset(Dataset):\n","#     def __init__(\n","#         self,\n","#         texts,\n","#         labels,\n","#         dictionary\n","#     ):\n","\n","#         self.texts = [torch.tensor([dictionary.token2id[word] for word in text]) for text in texts]\n","#         self.labels = labels\n","\n","#     def __getitem__(self, ind):\n","\n","#         instance_data = {\n","#             \"text\": self.texts[ind],\n","#             \"labels\": self.labels[ind],\n","#         }\n","\n","#         return instance_data\n","\n","#     def __len__(self):\n","#         return len(self.labels)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.722500Z","iopub.status.busy":"2024-10-17T15:16:50.722193Z","iopub.status.idle":"2024-10-17T15:16:50.731599Z","shell.execute_reply":"2024-10-17T15:16:50.730769Z","shell.execute_reply.started":"2024-10-17T15:16:50.722470Z"},"trusted":true},"outputs":[],"source":["# train_dataset = BaseDataset(tokenized, y_train, dictionary)\n","# # test_dataset = BaseDataset(texts_test, y_test, dictionary)"]},{"cell_type":"markdown","metadata":{},"source":["### продолжение"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.732998Z","iopub.status.busy":"2024-10-17T15:16:50.732719Z","iopub.status.idle":"2024-10-17T15:16:50.742434Z","shell.execute_reply":"2024-10-17T15:16:50.741540Z","shell.execute_reply.started":"2024-10-17T15:16:50.732969Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class BaseDataset2(Dataset):\n","    def __init__(\n","        self,\n","        texts,\n","        labels\n","    ):\n","\n","        self.texts = [torch.tensor(text.ids) for text in texts]\n","        self.labels = labels\n","\n","    def __getitem__(self, ind):\n","\n","        instance_data = {\n","            \"text\": self.texts[ind],\n","            \"labels\": self.labels[ind],\n","        }\n","\n","        return instance_data\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.747233Z","iopub.status.busy":"2024-10-17T15:16:50.746955Z","iopub.status.idle":"2024-10-17T15:16:51.475546Z","shell.execute_reply":"2024-10-17T15:16:51.474652Z","shell.execute_reply.started":"2024-10-17T15:16:50.747203Z"},"trusted":true},"outputs":[],"source":["train_dataset = BaseDataset2(tokenized_train, y_train)\n","test_dataset = BaseDataset2(tokenized_test, y_test)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.476923Z","iopub.status.busy":"2024-10-17T15:16:51.476639Z","iopub.status.idle":"2024-10-17T15:16:51.484542Z","shell.execute_reply":"2024-10-17T15:16:51.483335Z","shell.execute_reply.started":"2024-10-17T15:16:51.476893Z"},"trusted":true},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_fn(dataset_items: list[dict]):\n","\n","    result_batch = {}\n","\n","    dataset_items = sorted(dataset_items, key=lambda x: len(x[\"text\"]), reverse=True) # это было лишнее действие, думал получится код совместимым с packed_padded_sequence сделать\n","\n","    result_batch['lengths'] = [len(sample[\"text\"]) for sample in dataset_items]\n","    result_batch['texts'] = pad_sequence(\n","        [sample[\"text\"] for sample in dataset_items], batch_first=True\n","    )\n","\n","    result_batch['labels'] = torch.tensor([sample['labels'] for sample in dataset_items])\n","\n","\n","    return result_batch"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.486054Z","iopub.status.busy":"2024-10-17T15:16:51.485730Z","iopub.status.idle":"2024-10-17T15:16:51.494502Z","shell.execute_reply":"2024-10-17T15:16:51.493523Z","shell.execute_reply.started":"2024-10-17T15:16:51.486016Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=128,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n","    drop_last=True,\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=128,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n","    drop_last=False,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Метрика качества\n","\n","Перед тем, как приступить к обучению, нам нужно выбрать метрику оценки качества. Так как в задаче классификации с пересекающимися классами классы часто несбалансированы, чаще всего в качестве метрики берется [F1 score](https://en.wikipedia.org/wiki/F-score).\n","\n","Функция `compute_f1` принимает истинные метки и предсказанные и считает среднее значение F1 по всем классам. Используйте ее для оценки качества моделей.\n","\n","$$\n","F1_{total} = \\frac{1}{K} \\sum_{k=1}^K F1(Y_k, \\hat{Y}_k),\n","$$\n","где $Y_k$ – истинные значения для класса k, а $\\hat{Y}_k$ – предсказания."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.495917Z","iopub.status.busy":"2024-10-17T15:16:51.495637Z","iopub.status.idle":"2024-10-17T15:16:51.504250Z","shell.execute_reply":"2024-10-17T15:16:51.503403Z","shell.execute_reply.started":"2024-10-17T15:16:51.495886Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def compute_f1(y_true, y_pred):\n","    assert y_true.ndim == 2\n","    assert y_true.shape == y_pred.shape\n","\n","    return f1_score(y_true, y_pred, average='macro')"]},{"cell_type":"markdown","metadata":{"id":"aagj29J7Ap2H"},"source":["## Обучение моделей"]},{"cell_type":"markdown","metadata":{"id":"56ae5666"},"source":["### RNN\n","\n","В качестве бейзлайна обучим самую простую рекуррентную нейронную сеть. Напомним, что блок RNN выглядит таким образом.\n","\n","<img src=\"https://i.postimg.cc/yYbNBm6G/tg-image-1635618906.png\" alt=\"drawing\" width=\"400\"/>\n","\n","Его скрытое состояние обновляется по формуле\n","$h_t = \\sigma(W x_{t} + U h_{t-1} + b_h)$. А предсказание считается с помощью применения линейного слоя к последнему токену\n","$o_T = V h_T + b_o$. В качестве функции активации выберите гиперболический тангенс. \n","\n","__Задание 4 (2 балла)__. Реализуйте RNN в соответствии с формулой выше и обучите ее на нашу задачу. Нулевой скрытый вектор инициализируйте нулями, так модель будет обучаться стабильнее, чем при случайной инициализации. После этого замеряйте качество на тестовой выборке. У вас должно получиться значение F1 не меньше 0.33, а само обучение не должно занимать много времени."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.505880Z","iopub.status.busy":"2024-10-17T15:16:51.505507Z","iopub.status.idle":"2024-10-17T15:16:51.518247Z","shell.execute_reply":"2024-10-17T15:16:51.517143Z","shell.execute_reply.started":"2024-10-17T15:16:51.505837Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","from torch.nn import Sequential\n","\n","\n","class BaselineRNN(nn.Module):\n","    def __init__(self, src_vocab_size, input_size, hidden_dim, output_size):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(src_vocab_size, input_size)\n","\n","        self.W = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_h = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.V = nn.Linear(hidden_dim, output_size, bias=False)\n","        self.b_o = nn.Parameter(torch.zeros(output_size))\n","\n","        self.tanh = nn.Tanh()\n","        \n","\n","    def forward(self, texts, lengths, **batch):\n","        h0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","\n","        embedded_text = self.embedding(texts)\n","        all_outputs = []\n","\n","        for i in range(texts.shape[1]):\n","            h0 = self.tanh(self.U(h0) + self.W(embedded_text[:, i, :].squeeze(1)) + self.b_h) # <bs, output_size>\n","            all_outputs.append(self.V(h0) + self.b_o)\n","        \n","        last_outputs = []\n","        for i, length in enumerate(lengths):\n","            last_outputs.append(all_outputs[length - 1][i, :])\n","\n","        return torch.stack(last_outputs)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with the number of parameters.\n","        \"\"\"\n","        all_parameters = sum([p.numel() for p in self.parameters()])\n","        trainable_parameters = sum(\n","            [p.numel() for p in self.parameters() if p.requires_grad]\n","        )\n","\n","        result_info = super().__str__()\n","        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n","        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n","\n","        return result_info"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:25:28.018360Z","iopub.status.busy":"2024-10-17T15:25:28.016686Z","iopub.status.idle":"2024-10-17T15:25:28.046257Z","shell.execute_reply":"2024-10-17T15:25:28.045260Z","shell.execute_reply.started":"2024-10-17T15:25:28.018313Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","def train_epoch(net, train_loader, optimizer, lr_scheduler, criterion, wandb):\n","    losses = []\n","    possible_thresholds = [0.3, 0.6, 0.9, 0.95]\n","    f1_scores = {}\n","    for threshold in possible_thresholds:\n","        f1_scores[threshold] = []\n","\n","\n","    for batch in tqdm(train_loader, desc=\"train\", total=len(train_loader)):\n","        texts = torch.tensor(batch['texts']).to(device)\n","        lengths = torch.tensor(batch['lengths']).to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        out = net(texts, lengths).to('cpu')\n","        \n","        loss = criterion(out, batch['labels'])\n","        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5)\n","        loss.backward()\n","        losses.append(loss.detach().numpy())\n","\n","        optimizer.step()\n","        \n","        sigm_preds = torch.sigmoid(out)\n","        for threshold in possible_thresholds:\n","            f1 = compute_f1(batch['labels'], sigm_preds > threshold)\n","            f1_scores[threshold].append(f1)\n","\n","        if lr_scheduler is not None:\n","            wandb.log({'lr': lr_scheduler.get_lr()[0]})\n","            lr_scheduler.step()\n","            \n","        wandb.log({'train_CEloss': np.mean(losses),})\n","\n","    wandb.log(\n","        {f'train_f1_{threshold}': np.mean(f1_score) for threshold, f1_score in f1_scores.items()}\n","    )\n","\n","def save_model(wandb, net, epoch, optimizer, lr_scheduler, ):\n","    arch = type(net).__name__\n","    state = {\n","        \"arch\": arch,\n","        \"epoch\": epoch,\n","        \"state_dict\": net.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","        \"lr_scheduler\": lr_scheduler.state_dict(),\n","    }\n","    best_path = str(\"nlp_hw2_model_best.pth\")\n","    torch.save(state, best_path)\n","    wandb.save(best_path)\n","\n","def validate(net, test_loader, criterion, best_f1, wandb):\n","    with torch.no_grad():\n","        losses = []\n","        possible_thresholds = [0.3, 0.6, 0.9, 0.95]\n","        f1_scores = {}\n","        for threshold in possible_thresholds:\n","            f1_scores[threshold] = []\n","\n","        for batch in tqdm(test_loader, desc=\"test\", total=len(test_loader)):\n","            texts = torch.tensor(batch['texts']).to(device)\n","            lengths = torch.tensor(batch['lengths']).to(device)\n","            \n","            out = net(texts, lengths).to('cpu')\n","            \n","            loss = criterion(out, batch['labels'])\n","            losses.append(loss.detach().numpy())\n","            \n","            sigm_preds = torch.sigmoid(out)\n","            for threshold in possible_thresholds:\n","                f1 = compute_f1(batch['labels'], sigm_preds > threshold)\n","                f1_scores[threshold].append(f1)\n","\n","        wandb.log({\n","            'test_CEloss': np.mean(losses),\n","        })\n","        wandb.log(\n","            {f'test_f1_{threshold}': np.mean(f1_score) for threshold, f1_score in f1_scores.items()}\n","        )\n","\n","        if np.mean(f1_scores[0.6]) > best_f1:\n","            best_f1 = np.mean(f1_scores[0.6])\n","            return best_f1, True\n","        return best_f1, False\n","\n","\n","def train(net, train_loader, test_loader, optimizer, lr_scheduler, criterion, epochs, wandb):\n","    print(net)\n","    best_f1 = 0\n","    for epoch in range(epochs):\n","\n","        net.train()\n","        train_epoch(net, train_loader, optimizer, lr_scheduler, criterion, wandb)\n","\n","        net.eval()\n","        best_f1, save = validate(net, test_dataloader, criterion, best_f1, wandb)\n","        if save:\n","            print('Saving new best model...')\n","            save_model(wandb, net, epoch, optimizer, lr_scheduler)\n","\n","\n","def train_and_validate(wandb, tokenizer, train_dataloader, test_dataloader, model):\n","    # net = BaselineRNN(src_vocab_size=len(dictionary), input_size=16, hidden_dim=16, output_size=len(y_train[0])).to(device)\n","    net = model(src_vocab_size=tokenizer.get_vocab_size(), input_size=wandb.config.input_size, hidden_dim=wandb.config.hidden_dim, output_size=29).to(device)\n","    if torch.cuda.device_count() > 1:\n","        print('multiple gpus')\n","        net = nn.DataParallel(net)\n","    optimizer = torch.optim.AdamW(net.parameters())\n","    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=wandb.config.max_lr, pct_start=0.1, anneal_strategy='cos', steps_per_epoch=len(train_dataloader), epochs=wandb.config.epochs)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train(net, train_dataloader, test_dataloader, optimizer, lr_scheduler, criterion, wandb.config.epochs, wandb)"]},{"cell_type":"markdown","metadata":{"id":"xqt0dk6LEJUU"},"source":["### LSTM\n","\n","<img src=\"https://i.postimg.cc/pL5LdmpL/tg-image-2290675322.png\" alt=\"drawing\" width=\"400\"/>\n","\n","Теперь перейдем к более продвинутым рекурренным моделям, а именно LSTM. Из-за дополнительного вектора памяти эта модель должна гораздо лучше улавливать далекие зависимости, что должно напрямую отражаться на качестве.\n","\n","Параметры блока LSTM обновляются вот так ($\\sigma$ означает сигмоиду):\n","\\begin{align}\n","f_{t} &= \\sigma(W_f x_{t} + U_f h_{t-1} + b_f) \\\\ \n","i_{t} &= \\sigma(W_i x_{t} + U_i h_{t-1} + b_i) \\\\\n","\\tilde{c}_{t} &= \\tanh(W_c x_{t} + U_c h_{t-1} + b_i) \\\\\n","c_{t} &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n","o_{t} &= \\sigma(W_t x_{t} + U_t h_{t-1} + b_t) \\\\\n","h_t &= o_t \\odot \\tanh(c_t)\n","\\end{align}\n","\n","__Задание 5 (2 балла).__ Реализуйте LSTM по описанной схеме. Выберите гиперпараметры LSTM так, чтобы их общее число (без учета слоя эмбеддингов) примерно совпадало с числом параметров обычной RNN, но размерность скрытого слоя была не меньше 64. Так мы будем сравнивать архитектуры максимально независимо. Обучите LSTM до сходимости и сравните качество с RNN на тестовой выборке. Удалось ли получить лучший результат? Как вы можете это объяснить?"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from torch import nn\n","from torch.nn import Sequential\n","\n","\n","class LSTM(nn.Module):\n","    def __init__(self, src_vocab_size, input_size, hidden_dim, output_size):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(src_vocab_size, input_size)\n","\n","        self.W_f = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_f = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_i = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_i = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_c = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_o = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_t = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_t = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_t = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.V = nn.Linear(hidden_dim, output_size, bias=False)\n","        self.b_o = nn.Parameter(torch.zeros(output_size))\n","\n","        self.tanh = nn.Tanh()\n","        self.sigm = nn.Sigmoid()\n","        \n","\n","    def forward(self, texts, lengths, **batch):\n","        h0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","        c0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","\n","        embedded_text = self.embedding(texts)\n","        all_outputs = []\n","\n","        for i in range(texts.shape[1]):\n","            x_i = embedded_text[:, i, :].squeeze(1)\n","            ft = self.sigm(self.W_f(x_i) + self.U_f(h0) + self.b_f)\n","            it = self.sigm(self.W_i(x_i) + self.U_i(h0) + self.b_i)\n","            wave_ct = self.tanh(self.W_c(x_i) + self.U_c(h0) + self.b_c)\n","            c0 = ft * c0 + it * wave_ct\n","            ot = self.sigm(self.W_t(x_i) + self.U_t(h0) + self.b_t)\n","            h0 = ot * self.tanh(c0)\n","\n","            all_outputs.append(self.V(h0) + self.b_o)\n","        \n","        last_outputs = []\n","        for i, length in enumerate(lengths):\n","            last_outputs.append(all_outputs[length - 1][i, :])\n","\n","        return torch.stack(last_outputs)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with the number of parameters.\n","        \"\"\"\n","        all_parameters = sum([p.numel() for p in self.parameters()])\n","        trainable_parameters = sum(\n","            [p.numel() for p in self.parameters() if p.requires_grad]\n","        )\n","\n","        result_info = super().__str__()\n","        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n","        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n","\n","        return result_info"]},{"cell_type":"code","execution_count":38,"id":"5fc7ed7b","metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:zugr94d8) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">testing</strong> at: <a href='https://wandb.ai/mate-ball/nlp_hw2_rnn/runs/zugr94d8' target=\"_blank\">https://wandb.ai/mate-ball/nlp_hw2_rnn/runs/zugr94d8</a><br/> View project at: <a href='https://wandb.ai/mate-ball/nlp_hw2_rnn' target=\"_blank\">https://wandb.ai/mate-ball/nlp_hw2_rnn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241017_195812-zugr94d8/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:zugr94d8). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/bspanfilov/Documents/edu/hse-4/nlp/hw2_rnn/wandb/run-20241017_195828-6e4to2hz</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/mate-ball/nlp_hw2_rnn/runs/6e4to2hz' target=\"_blank\">testing</a></strong> to <a href='https://wandb.ai/mate-ball/nlp_hw2_rnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/mate-ball/nlp_hw2_rnn' target=\"_blank\">https://wandb.ai/mate-ball/nlp_hw2_rnn</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/mate-ball/nlp_hw2_rnn/runs/6e4to2hz' target=\"_blank\">https://wandb.ai/mate-ball/nlp_hw2_rnn/runs/6e4to2hz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["BaselineRNN(\n","  (embedding): Embedding(16000, 128)\n","  (W): Linear(in_features=128, out_features=256, bias=False)\n","  (U): Linear(in_features=256, out_features=256, bias=False)\n","  (V): Linear(in_features=256, out_features=29, bias=False)\n","  (tanh): Tanh()\n",")\n","All parameters: 2154013\n","Trainable parameters: 2154013\n"]},{"name":"stderr","output_type":"stream","text":["train:   0%|          | 0/18 [01:49<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      2\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp_hw2_rnn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaselineRNN\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[33], line 115\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(wandb, tokenizer, train_dataloader, test_dataloader, model)\u001b[0m\n\u001b[1;32m    112\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(optimizer, max_lr\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_lr, pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, anneal_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m'\u001b[39m, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), epochs\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m    113\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> 115\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[33], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, test_loader, optimizer, lr_scheduler, criterion, epochs, wandb)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     95\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 96\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     99\u001b[0m     best_f1, save \u001b[38;5;241m=\u001b[39m validate(net, test_dataloader, criterion, best_f1, wandb)\n","Cell \u001b[0;32mIn[33], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(net, train_loader, optimizer, lr_scheduler, criterion, wandb)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m~/.pyenv/versions/3.9.19/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.9.19/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"]}],"source":["wandb.init(\n","    project=\"nlp_hw2_rnn\",\n","    name='testing',\n","\n","    config={\n","    \"max_lr\": 5e-3,\n","    \"input_size\": 128,\n","    \"hidden_dim\": 256,\n","    \"epochs\": 30,\n","    }\n",")\n","\n","train_and_validate(wandb, tokenizer, train_dataloader, test_dataloader, BaselineRNN)"]},{"cell_type":"markdown","metadata":{"id":"phQ-ka4mp0oS"},"source":["__Задание 6 (1 балл).__ В этом задании у вас есть две опции на выбор: добавить __двунаправленность__ для LSTM _или_ добавить __многослойность__. Можно сделать и то, и другое, но дополнительных баллов за это мы не дадим, только бесконечный респект. Обе модификации реализуются довольно просто (буквально 4 строчки кода, если вы аккуратно реализовали модель) и дают примерно одинаковый прирост в качестве. Сделайте выводы: стоит ли увеличивать размер модели в несколько раз?"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ee7c7177"},"outputs":[],"source":["# your code here"]},{"cell_type":"code","execution_count":37,"id":"87a8c6c8","metadata":{},"outputs":[{"data":{"text/plain":["2482461"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["2482461"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"widgets":{"application/vnd.jupyter.widget-state+json":{"12b0627d4aaf46c0adc64b442bf88d0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d5b2e090c51406e953b4eec4b0b91ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"282f83858a424e2ea76990eb957dc5a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32808478ae8c4242beb79f0272ea6b1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34e8d1401c0e4dc1a8e71bbad7c2f74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23f3b8b7247491c8d5e3ead7f54d886","placeholder":"​","style":"IPY_MODEL_cb632291897f4f9db86a00a5a71ca35f","value":" 40/40 [36:41&lt;00:00, 51.61s/it]"}},"3735627f227d4b4f927955113111409f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47f4f11bc6984b96ac3c3875d733f0ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4f687f9d5940aba074e2bb41581c93","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e10fd6d1a6c47a9ac34a47ae5ba708b","value":40}},"4aab16bb20824688aadbd23460adad9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f65eec1b45de42e59fb9e24b99aad917","IPY_MODEL_47f4f11bc6984b96ac3c3875d733f0ba","IPY_MODEL_f58fddb1bf414071b0523701a619ad71"],"layout":"IPY_MODEL_32808478ae8c4242beb79f0272ea6b1f"}},"4de9492961d841aa9f3d7bc629911296":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ae0c089c4a426db3b52976fae1a9dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e10fd6d1a6c47a9ac34a47ae5ba708b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b23f3b8b7247491c8d5e3ead7f54d886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4165ff8fc3480fb1590b6ecd39fb4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cba16e32a9df4b1b89b4f7066945fc42","IPY_MODEL_e8f0522f19c44066b5a78ded999f050a","IPY_MODEL_34e8d1401c0e4dc1a8e71bbad7c2f74d"],"layout":"IPY_MODEL_282f83858a424e2ea76990eb957dc5a0"}},"cb632291897f4f9db86a00a5a71ca35f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cba16e32a9df4b1b89b4f7066945fc42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ae0c089c4a426db3b52976fae1a9dc","placeholder":"​","style":"IPY_MODEL_12b0627d4aaf46c0adc64b442bf88d0a","value":"100%"}},"d7ed88f49793494bbdb3c2fffc01b216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4f687f9d5940aba074e2bb41581c93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7876fd73da349ea873c137c63d8d528":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8f0522f19c44066b5a78ded999f050a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4de9492961d841aa9f3d7bc629911296","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7876fd73da349ea873c137c63d8d528","value":40}},"f58fddb1bf414071b0523701a619ad71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d5b2e090c51406e953b4eec4b0b91ad","placeholder":"​","style":"IPY_MODEL_3735627f227d4b4f927955113111409f","value":" 40/40 [1:08:10&lt;00:00, 102.17s/it]"}},"f65eec1b45de42e59fb9e24b99aad917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f67dc08a01ac40ad98ed553fe6b7e948","placeholder":"​","style":"IPY_MODEL_d7ed88f49793494bbdb3c2fffc01b216","value":"100%"}},"f67dc08a01ac40ad98ed553fe6b7e948":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":5}
