{"cells":[{"cell_type":"markdown","metadata":{"id":"b1acf78a"},"source":["# Глубинное обучение для текстовых данных, ФКН ВШЭ\n","\n","## Домашнее задание 2: Рекуррентные нейронные сети\n","\n","### Оценивание и штрафы\n","\n","Максимально допустимая оценка за работу — __10 (+5) баллов__. Сдавать задание после указанного срока сдачи нельзя.\n","\n","Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n","\n","Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n","\n","__Мягкий дедлайн: 14.10.24 23:59__   \n","__Жесткий дедлайн: 17.10.24 23:59__\n","\n","### О задании\n","\n","В этом задании вам предстоит самостоятельно реализовать модель LSTM для решения задачи классификации с пересекающимися классами (multi-label classification). Это вид классификации, в которой каждый объект может относиться одновременно к нескольким классам. Такая задача часто возникает при классификации фильмов по жанрам, научных или новостных статей по темам, музыкальных композиций по инструментам и так далее.\n","\n","В нашем случае мы будем работать с датасетом биотехнических новостей и классифицировать их по темам. Этот датасет уже предобработан: текст приведен к нижнему регистру, удалена пунктуация, все слова разделены проблелом."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:24.396842Z","iopub.status.busy":"2024-10-17T15:16:24.396470Z","iopub.status.idle":"2024-10-17T15:16:45.019183Z","shell.execute_reply":"2024-10-17T15:16:45.018202Z","shell.execute_reply.started":"2024-10-17T15:16:24.396807Z"},"trusted":true},"outputs":[],"source":["%pip install gdown\n","!gdown https://drive.google.com/uc?id=1OCbRPznUPXmj9IC410HzL4VldgUhXZCm"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.021535Z","iopub.status.busy":"2024-10-17T15:16:45.021170Z","iopub.status.idle":"2024-10-17T15:16:45.083397Z","shell.execute_reply":"2024-10-17T15:16:45.082664Z","shell.execute_reply.started":"2024-10-17T15:16:45.021500Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/bspanfilov/.netrc\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import re\n","import random\n","import os\n","from nltk.corpus import stopwords\n","import torch\n","import wandb\n","import warnings\n","from collections import defaultdict\n","warnings.filterwarnings(\"ignore\")\n","\n","wandb.login()\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.084647Z","iopub.status.busy":"2024-10-17T15:16:45.084355Z","iopub.status.idle":"2024-10-17T15:16:45.091723Z","shell.execute_reply":"2024-10-17T15:16:45.090871Z","shell.execute_reply.started":"2024-10-17T15:16:45.084616Z"},"trusted":true},"outputs":[],"source":["def set_random_seed(seed):\n","    \"\"\"\n","    Set random seed for model training or inference.\n","\n","    Args:\n","        seed (int): defines which seed to use.\n","    \"\"\"\n","    # fix random seeds for reproducibility\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    # benchmark=True works faster but reproducibility decreases\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","set_random_seed(1)"]},{"cell_type":"code","execution_count":141,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.execute_input":"2024-10-17T15:16:45.094096Z","iopub.status.busy":"2024-10-17T15:16:45.093802Z","iopub.status.idle":"2024-10-17T15:16:45.210310Z","shell.execute_reply":"2024-10-17T15:16:45.209284Z","shell.execute_reply.started":"2024-10-17T15:16:45.094065Z"},"id":"af1a5fff","outputId":"891c58cd-6964-4319-ade3-92bb90356f93","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drive your plow over the bones of the dead by ...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in the recently tabled national budget denel h...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>shares take a break its good for you picture g...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>reso is currently hiring for two positions pro...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>charter buyer club what is the charter buyer c...</td>\n","      <td>other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text labels\n","0  drive your plow over the bones of the dead by ...  other\n","1  in the recently tabled national budget denel h...  other\n","2  shares take a break its good for you picture g...  other\n","3  reso is currently hiring for two positions pro...  other\n","4  charter buyer club what is the charter buyer c...  other"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["dataset = pd.read_csv('biotech_news.tsv', sep='\\t')\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"HRBZwYd9QMMS"},"source":["## Предобработка лейблов\n","\n","\n","__Задание 1 (1.5 балла)__. Как вы можете заметить, лейблы записаны в виде строк, разделенных запятыми. Для работы с ними нам нужно преобразовать их в числа. Так как каждый объект может принадлежать нескольким классам, закодируйте лейблы в виде векторов из 0 и 1, где 1 означает, что объект принадлежит соответствующему классу, а 0 – не принадлежит. Имея такую кодировку, мы сможем обучить модель, решая задачу бинарной классификации для каждого класса."]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.212125Z","iopub.status.busy":"2024-10-17T15:16:45.211705Z","iopub.status.idle":"2024-10-17T15:16:45.222337Z","shell.execute_reply":"2024-10-17T15:16:45.221345Z","shell.execute_reply.started":"2024-10-17T15:16:45.212077Z"},"trusted":true},"outputs":[],"source":["labels = set()\n","for label_lst in dataset.labels.unique():\n","    for label in label_lst.split(', '):\n","        labels.add(label)"]},{"cell_type":"code","execution_count":143,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.223970Z","iopub.status.busy":"2024-10-17T15:16:45.223589Z","iopub.status.idle":"2024-10-17T15:16:45.230472Z","shell.execute_reply":"2024-10-17T15:16:45.229603Z","shell.execute_reply.started":"2024-10-17T15:16:45.223925Z"},"trusted":true},"outputs":[],"source":["label_to_num = {v:k for k, v in enumerate(labels)}"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.232032Z","iopub.status.busy":"2024-10-17T15:16:45.231668Z","iopub.status.idle":"2024-10-17T15:16:45.241036Z","shell.execute_reply":"2024-10-17T15:16:45.240058Z","shell.execute_reply.started":"2024-10-17T15:16:45.231990Z"},"trusted":true},"outputs":[],"source":["def get_labels_array(labels: str):\n","    target = np.zeros(len(label_to_num))\n","    for label in labels.split(', '):\n","        target[label_to_num[label]] = 1\n","\n","    return target"]},{"cell_type":"code","execution_count":145,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.242589Z","iopub.status.busy":"2024-10-17T15:16:45.242219Z","iopub.status.idle":"2024-10-17T15:16:45.258729Z","shell.execute_reply":"2024-10-17T15:16:45.258041Z","shell.execute_reply.started":"2024-10-17T15:16:45.242548Z"},"trusted":true},"outputs":[],"source":["dataset['num_labels'] = dataset['labels'].apply(get_labels_array)"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.259911Z","iopub.status.busy":"2024-10-17T15:16:45.259665Z","iopub.status.idle":"2024-10-17T15:16:45.276607Z","shell.execute_reply":"2024-10-17T15:16:45.275694Z","shell.execute_reply.started":"2024-10-17T15:16:45.259883Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>num_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drive your plow over the bones of the dead by ...</td>\n","      <td>other</td>\n","      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in the recently tabled national budget denel h...</td>\n","      <td>other</td>\n","      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>shares take a break its good for you picture g...</td>\n","      <td>other</td>\n","      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text labels  \\\n","0  drive your plow over the bones of the dead by ...  other   \n","1  in the recently tabled national budget denel h...  other   \n","2  shares take a break its good for you picture g...  other   \n","\n","                                          num_labels  \n","0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","1  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Предобработка данных"]},{"cell_type":"markdown","metadata":{"id":"vMe0c5AAXM8d"},"source":["В этом задании мы будем обучать рекуррентные нейронные сети. Как вы знаете, они работают лучше для коротких текстов, так как не очень хорошо улавливают далекие зависимости. Для уменьшение длин текстов их стоит почистить.\n","\n","Сразу разделим выборку на обучающую и тестовую, чтобы считать все нужные статистики только по обучающей."]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.280309Z","iopub.status.busy":"2024-10-17T15:16:45.280012Z","iopub.status.idle":"2024-10-17T15:16:45.289621Z","shell.execute_reply":"2024-10-17T15:16:45.288776Z","shell.execute_reply.started":"2024-10-17T15:16:45.280279Z"},"id":"f8135000","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","texts_train, texts_test, y_train, y_test = train_test_split(\n","    dataset['text'].to_numpy(),\n","    dataset['num_labels'].to_numpy(),\n","    test_size=0.2,  # do not change this\n","    random_state=0  # do not change this\n",")"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.290971Z","iopub.status.busy":"2024-10-17T15:16:45.290678Z","iopub.status.idle":"2024-10-17T15:16:45.340647Z","shell.execute_reply":"2024-10-17T15:16:45.339782Z","shell.execute_reply.started":"2024-10-17T15:16:45.290940Z"},"trusted":true},"outputs":[],"source":["new_y_train = np.zeros((len(y_train), len(y_train[0])))\n","\n","for i in range(len(y_train)):\n","    for j in range(len(y_train[0])):\n","        new_y_train[i, j] = y_train[i][j]\n","\n","y_train = new_y_train"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.341903Z","iopub.status.busy":"2024-10-17T15:16:45.341662Z","iopub.status.idle":"2024-10-17T15:16:45.357957Z","shell.execute_reply":"2024-10-17T15:16:45.357106Z","shell.execute_reply.started":"2024-10-17T15:16:45.341876Z"},"trusted":true},"outputs":[],"source":["new_y_test = np.zeros((len(y_test), len(y_test[0])))\n","\n","for i in range(len(y_test)):\n","    for j in range(len(y_test[0])):\n","        new_y_test[i, j] = y_test[i][j]\n","\n","y_test = new_y_test"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.359608Z","iopub.status.busy":"2024-10-17T15:16:45.359132Z","iopub.status.idle":"2024-10-17T15:16:45.372217Z","shell.execute_reply":"2024-10-17T15:16:45.371420Z","shell.execute_reply.started":"2024-10-17T15:16:45.359557Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2431,), (2431, 29), (608,), (608, 29))"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["texts_train.shape, y_train.shape, texts_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["__Задание 2 (1.5 балла)__. Удалите из текстов стоп слова, слишком редкие и слишком частые слова. Гиперпараметры подберите самостоятельно (в идеале их стоит подбирать по качеству на тестовой выборке). Если вы считаете, что стоит добавить еще какую-то обработку, то сделайте это. Важно не удалить ничего, что может повлиять на предсказание класса."]},{"cell_type":"code","execution_count":151,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.373440Z","iopub.status.busy":"2024-10-17T15:16:45.373161Z","iopub.status.idle":"2024-10-17T15:16:45.388504Z","shell.execute_reply":"2024-10-17T15:16:45.387734Z","shell.execute_reply.started":"2024-10-17T15:16:45.373408Z"},"id":"BcmyCcoaXIqy","trusted":true},"outputs":[],"source":["stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":152,"id":"1c395f1a","metadata":{},"outputs":[],"source":["word_to_num = defaultdict(int)\n","for text in texts_train:\n","    text = text.split()\n","    for word in text:\n","        word_to_num[word] += 1"]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.390102Z","iopub.status.busy":"2024-10-17T15:16:45.389760Z","iopub.status.idle":"2024-10-17T15:16:45.394605Z","shell.execute_reply":"2024-10-17T15:16:45.393711Z","shell.execute_reply.started":"2024-10-17T15:16:45.390068Z"},"trusted":true},"outputs":[],"source":["def remove_stopwords(text):\n","    clear_texts = []\n","    clear_text = [word for word in text.split(' ') if word not in stop_words and 7 < word_to_num[word] < 2500]\n","\n","    return ' '.join(clear_text)"]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:45.396146Z","iopub.status.busy":"2024-10-17T15:16:45.395800Z","iopub.status.idle":"2024-10-17T15:16:48.479473Z","shell.execute_reply":"2024-10-17T15:16:48.478423Z","shell.execute_reply.started":"2024-10-17T15:16:45.396104Z"},"trusted":true},"outputs":[],"source":["texts_train = np.array([remove_stopwords(text) for text in texts_train])\n","texts_test = np.array([remove_stopwords(text) for text in texts_test])"]},{"cell_type":"code","execution_count":155,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:48.481409Z","iopub.status.busy":"2024-10-17T15:16:48.480997Z","iopub.status.idle":"2024-10-17T15:16:48.489496Z","shell.execute_reply":"2024-10-17T15:16:48.488626Z","shell.execute_reply.started":"2024-10-17T15:16:48.481339Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2431,), (2431, 29), (608,), (608, 29))"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["texts_train.shape, y_train.shape, texts_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["__Задание 3 (2 балла)__. Осталось перевести тексты в индексы токенов, чтобы их можно было подавать в модель. У вас есть две опции, как это сделать:\n","1. __(+0 баллов)__ Токенизировать тексты по словам.\n","2. __(до +5 баллов)__ Реализовать свою токенизацию BPE. Количество баллов будет варьироваться в зависимости от эффективности реализации. При реализации нельзя пользоваться специализированными библиотеками.\n","\n","Токенизируйте тексты, переведите их в списки индексов и сложите вместе с лейблами в `DataLoader`. Не забудьте добавить в `DataLoader` `collate_fn`, которая будет дополнять все короткие тексты в батче паддингами. Для маппинга токенов в индексы вам может пригодиться `gensim.corpora.dictionary.Dictionary`."]},{"cell_type":"code","execution_count":156,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:48.490860Z","iopub.status.busy":"2024-10-17T15:16:48.490589Z","iopub.status.idle":"2024-10-17T15:16:48.503229Z","shell.execute_reply":"2024-10-17T15:16:48.502410Z","shell.execute_reply.started":"2024-10-17T15:16:48.490831Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import time\n","from IPython.display import clear_output\n","\n","\n","def word_tokenizer(texts: list[str]) -> list[list]:\n","    return [text.split(' ') for text in texts]\n"]},{"cell_type":"code","execution_count":157,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:49.647323Z","iopub.status.busy":"2024-10-17T15:16:49.647005Z","iopub.status.idle":"2024-10-17T15:16:50.676738Z","shell.execute_reply":"2024-10-17T15:16:50.675913Z","shell.execute_reply.started":"2024-10-17T15:16:49.647289Z"},"trusted":true},"outputs":[],"source":["tokenized_train = word_tokenizer(texts_train)\n","tokenized_test = word_tokenizer(texts_test)"]},{"cell_type":"code","execution_count":158,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.678790Z","iopub.status.busy":"2024-10-17T15:16:50.678049Z","iopub.status.idle":"2024-10-17T15:16:50.683046Z","shell.execute_reply":"2024-10-17T15:16:50.682138Z","shell.execute_reply.started":"2024-10-17T15:16:50.678744Z"},"trusted":true},"outputs":[],"source":["import gensim\n","from gensim.corpora.dictionary import Dictionary\n","\n","dictionary = Dictionary(word_tokenizer(np.concatenate((texts_train, texts_test))))\n"]},{"cell_type":"code","execution_count":159,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.712119Z","iopub.status.busy":"2024-10-17T15:16:50.711778Z","iopub.status.idle":"2024-10-17T15:16:50.721110Z","shell.execute_reply":"2024-10-17T15:16:50.720354Z","shell.execute_reply.started":"2024-10-17T15:16:50.712069Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class BaseDataset(Dataset):\n","    def __init__(\n","        self,\n","        texts,\n","        labels,\n","        dictionary\n","    ):\n","\n","        self.texts = [torch.tensor([dictionary.token2id[word] for word in text]) for text in texts]\n","        self.labels = labels\n","\n","    def __getitem__(self, ind):\n","\n","        instance_data = {\n","            \"text\": self.texts[ind],\n","            \"labels\": self.labels[ind],\n","        }\n","\n","        return instance_data\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":160,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:50.722500Z","iopub.status.busy":"2024-10-17T15:16:50.722193Z","iopub.status.idle":"2024-10-17T15:16:50.731599Z","shell.execute_reply":"2024-10-17T15:16:50.730769Z","shell.execute_reply.started":"2024-10-17T15:16:50.722470Z"},"trusted":true},"outputs":[],"source":["train_dataset = BaseDataset(tokenized_train, y_train, dictionary)\n","test_dataset = BaseDataset(tokenized_test, y_test, dictionary)"]},{"cell_type":"code","execution_count":161,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.476923Z","iopub.status.busy":"2024-10-17T15:16:51.476639Z","iopub.status.idle":"2024-10-17T15:16:51.484542Z","shell.execute_reply":"2024-10-17T15:16:51.483335Z","shell.execute_reply.started":"2024-10-17T15:16:51.476893Z"},"trusted":true},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_fn(dataset_items: list[dict]):\n","\n","    result_batch = {}\n","\n","    dataset_items = sorted(dataset_items, key=lambda x: len(x[\"text\"]), reverse=True) # это было лишнее действие, думал получится код совместимым с packed_padded_sequence сделать\n","\n","    result_batch['lengths'] = [len(sample[\"text\"]) for sample in dataset_items]\n","    result_batch['texts'] = pad_sequence(\n","        [sample[\"text\"] for sample in dataset_items], batch_first=True\n","    )\n","\n","    result_batch['labels'] = torch.tensor([sample['labels'] for sample in dataset_items])\n","\n","\n","    return result_batch"]},{"cell_type":"code","execution_count":162,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.486054Z","iopub.status.busy":"2024-10-17T15:16:51.485730Z","iopub.status.idle":"2024-10-17T15:16:51.494502Z","shell.execute_reply":"2024-10-17T15:16:51.493523Z","shell.execute_reply.started":"2024-10-17T15:16:51.486016Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=128,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n","    drop_last=True,\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=128,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    pin_memory=True,\n","    drop_last=False,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Метрика качества\n","\n","Перед тем, как приступить к обучению, нам нужно выбрать метрику оценки качества. Так как в задаче классификации с пересекающимися классами классы часто несбалансированы, чаще всего в качестве метрики берется [F1 score](https://en.wikipedia.org/wiki/F-score).\n","\n","Функция `compute_f1` принимает истинные метки и предсказанные и считает среднее значение F1 по всем классам. Используйте ее для оценки качества моделей.\n","\n","$$\n","F1_{total} = \\frac{1}{K} \\sum_{k=1}^K F1(Y_k, \\hat{Y}_k),\n","$$\n","где $Y_k$ – истинные значения для класса k, а $\\hat{Y}_k$ – предсказания."]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.495917Z","iopub.status.busy":"2024-10-17T15:16:51.495637Z","iopub.status.idle":"2024-10-17T15:16:51.504250Z","shell.execute_reply":"2024-10-17T15:16:51.503403Z","shell.execute_reply.started":"2024-10-17T15:16:51.495886Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def compute_f1(y_true, y_pred):\n","    assert y_true.ndim == 2\n","    assert y_true.shape == y_pred.shape\n","\n","    return f1_score(y_true, y_pred, average='macro')"]},{"cell_type":"markdown","metadata":{"id":"aagj29J7Ap2H"},"source":["## Обучение моделей"]},{"cell_type":"markdown","metadata":{"id":"56ae5666"},"source":["### RNN\n","\n","В качестве бейзлайна обучим самую простую рекуррентную нейронную сеть. Напомним, что блок RNN выглядит таким образом.\n","\n","<img src=\"https://i.postimg.cc/yYbNBm6G/tg-image-1635618906.png\" alt=\"drawing\" width=\"400\"/>\n","\n","Его скрытое состояние обновляется по формуле\n","$h_t = \\sigma(W x_{t} + U h_{t-1} + b_h)$. А предсказание считается с помощью применения линейного слоя к последнему токену\n","$o_T = V h_T + b_o$. В качестве функции активации выберите гиперболический тангенс. \n","\n","__Задание 4 (2 балла)__. Реализуйте RNN в соответствии с формулой выше и обучите ее на нашу задачу. Нулевой скрытый вектор инициализируйте нулями, так модель будет обучаться стабильнее, чем при случайной инициализации. После этого замеряйте качество на тестовой выборке. У вас должно получиться значение F1 не меньше 0.33, а само обучение не должно занимать много времени."]},{"cell_type":"code","execution_count":164,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:16:51.505880Z","iopub.status.busy":"2024-10-17T15:16:51.505507Z","iopub.status.idle":"2024-10-17T15:16:51.518247Z","shell.execute_reply":"2024-10-17T15:16:51.517143Z","shell.execute_reply.started":"2024-10-17T15:16:51.505837Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","from torch.nn import Sequential\n","\n","\n","class BaselineRNN(nn.Module):\n","    def __init__(self, src_vocab_size, input_size, hidden_dim, output_size):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(src_vocab_size, input_size)\n","\n","        self.W = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_h = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.V = nn.Linear(hidden_dim, output_size, bias=False)\n","        self.b_o = nn.Parameter(torch.zeros(output_size))\n","\n","        self.tanh = nn.Tanh()\n","        \n","\n","    def forward(self, texts, lengths, **batch):\n","        h0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","\n","        embedded_text = self.embedding(texts)\n","        all_outputs = []\n","\n","        for i in range(texts.shape[1]):\n","            h0 = self.tanh(self.U(h0) + self.W(embedded_text[:, i, :].squeeze(1)) + self.b_h) # <bs, output_size>\n","            all_outputs.append(self.V(h0) + self.b_o)\n","        \n","        last_outputs = []\n","        for i, length in enumerate(lengths):\n","            last_outputs.append(all_outputs[length - 1][i, :])\n","\n","        return torch.stack(last_outputs)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with the number of parameters.\n","        \"\"\"\n","        all_parameters = sum([p.numel() for p in self.parameters()])\n","        trainable_parameters = sum(\n","            [p.numel() for p in self.parameters() if p.requires_grad]\n","        )\n","\n","        result_info = super().__str__()\n","        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n","        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n","\n","        return result_info"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T15:25:28.018360Z","iopub.status.busy":"2024-10-17T15:25:28.016686Z","iopub.status.idle":"2024-10-17T15:25:28.046257Z","shell.execute_reply":"2024-10-17T15:25:28.045260Z","shell.execute_reply.started":"2024-10-17T15:25:28.018313Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","def train_epoch(net, train_loader, optimizer, lr_scheduler, criterion, wandb):\n","    losses = []\n","    possible_thresholds = [0.3, 0.6, 0.9, 0.95]\n","    f1_scores = {}\n","    for threshold in possible_thresholds:\n","        f1_scores[threshold] = []\n","\n","\n","    for batch in tqdm(train_loader, desc=\"train\", total=len(train_loader)):\n","        texts = torch.tensor(batch['texts']).to(device)\n","        lengths = torch.tensor(batch['lengths']).to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        out = net(texts, lengths).to('cpu')\n","        \n","        loss = criterion(out, batch['labels'])\n","        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5)\n","        loss.backward()\n","        losses.append(loss.detach().numpy())\n","\n","        optimizer.step()\n","        \n","        sigm_preds = torch.sigmoid(out)\n","        for threshold in possible_thresholds:\n","            f1 = compute_f1(batch['labels'], sigm_preds > threshold)\n","            f1_scores[threshold].append(f1)\n","\n","        if lr_scheduler is not None:\n","            wandb.log({'lr': lr_scheduler.get_lr()[0]})\n","            lr_scheduler.step()\n","            \n","        wandb.log({'train_CEloss': np.mean(losses),})\n","\n","    wandb.log(\n","        {f'train_f1_{threshold}': np.mean(f1_score) for threshold, f1_score in f1_scores.items()}\n","    )\n","\n","def save_model(wandb, net, epoch, optimizer, lr_scheduler, ):\n","    arch = type(net).__name__\n","    state = {\n","        \"arch\": arch,\n","        \"epoch\": epoch,\n","        \"state_dict\": net.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","        \"lr_scheduler\": lr_scheduler.state_dict(),\n","    }\n","    best_path = str(\"nlp_hw2_model_best.pth\")\n","    torch.save(state, best_path)\n","    wandb.save(best_path)\n","\n","def validate(net, test_loader, criterion, best_f1, wandb):\n","    with torch.no_grad():\n","        losses = []\n","        possible_thresholds = [0.3, 0.6, 0.9, 0.95]\n","        f1_scores = {}\n","        for threshold in possible_thresholds:\n","            f1_scores[threshold] = []\n","\n","        for batch in tqdm(test_loader, desc=\"test\", total=len(test_loader)):\n","            texts = torch.tensor(batch['texts']).to(device)\n","            lengths = torch.tensor(batch['lengths']).to(device)\n","            \n","            out = net(texts, lengths).to('cpu')\n","            \n","            loss = criterion(out, batch['labels'])\n","            losses.append(loss.detach().numpy())\n","            \n","            sigm_preds = torch.sigmoid(out)\n","            for threshold in possible_thresholds:\n","                f1 = compute_f1(batch['labels'], sigm_preds > threshold)\n","                f1_scores[threshold].append(f1)\n","\n","        wandb.log({\n","            'test_CEloss': np.mean(losses),\n","        })\n","        wandb.log(\n","            {f'test_f1_{threshold}': np.mean(f1_score) for threshold, f1_score in f1_scores.items()}\n","        )\n","\n","        if np.mean(f1_scores[0.6]) > best_f1:\n","            best_f1 = np.mean(f1_scores[0.6])\n","            return best_f1, True\n","        return best_f1, False\n","\n","\n","def train(net, train_loader, test_loader, optimizer, lr_scheduler, criterion, epochs, wandb):\n","    print(net)\n","    best_f1 = 0\n","    for epoch in range(epochs):\n","\n","        net.train()\n","        train_epoch(net, train_loader, optimizer, lr_scheduler, criterion, wandb)\n","\n","        net.eval()\n","        best_f1, save = validate(net, test_dataloader, criterion, best_f1, wandb)\n","        if save:\n","            print('Saving new best model...')\n","            save_model(wandb, net, epoch, optimizer, lr_scheduler)\n","\n","\n","def train_and_validate(wandb, vocab_size, train_dataloader, test_dataloader, model):\n","    net = model(src_vocab_size=vocab_size, input_size=wandb.config.input_size, hidden_dim=wandb.config.hidden_dim, output_size=29).to(device)\n","    if torch.cuda.device_count() > 1:\n","        print('multiple gpus')\n","        net = nn.DataParallel(net)\n","    optimizer = torch.optim.AdamW(net.parameters())\n","    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=wandb.config.max_lr, pct_start=0.1, anneal_strategy='cos', steps_per_epoch=len(train_dataloader), epochs=wandb.config.epochs)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train(net, train_dataloader, test_dataloader, optimizer, lr_scheduler, criterion, wandb.config.epochs, wandb)"]},{"cell_type":"markdown","metadata":{"id":"xqt0dk6LEJUU"},"source":["### LSTM\n","\n","<img src=\"https://i.postimg.cc/pL5LdmpL/tg-image-2290675322.png\" alt=\"drawing\" width=\"400\"/>\n","\n","Теперь перейдем к более продвинутым рекурренным моделям, а именно LSTM. Из-за дополнительного вектора памяти эта модель должна гораздо лучше улавливать далекие зависимости, что должно напрямую отражаться на качестве.\n","\n","Параметры блока LSTM обновляются вот так ($\\sigma$ означает сигмоиду):\n","\\begin{align}\n","f_{t} &= \\sigma(W_f x_{t} + U_f h_{t-1} + b_f) \\\\ \n","i_{t} &= \\sigma(W_i x_{t} + U_i h_{t-1} + b_i) \\\\\n","\\tilde{c}_{t} &= \\tanh(W_c x_{t} + U_c h_{t-1} + b_i) \\\\\n","c_{t} &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n","o_{t} &= \\sigma(W_t x_{t} + U_t h_{t-1} + b_t) \\\\\n","h_t &= o_t \\odot \\tanh(c_t)\n","\\end{align}\n","\n","__Задание 5 (2 балла).__ Реализуйте LSTM по описанной схеме. Выберите гиперпараметры LSTM так, чтобы их общее число (без учета слоя эмбеддингов) примерно совпадало с числом параметров обычной RNN, но размерность скрытого слоя была не меньше 64. Так мы будем сравнивать архитектуры максимально независимо. Обучите LSTM до сходимости и сравните качество с RNN на тестовой выборке. Удалось ли получить лучший результат? Как вы можете это объяснить?"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[],"source":["from torch import nn\n","from torch.nn import Sequential\n","\n","\n","class LSTMNet(nn.Module):\n","    def __init__(self, src_vocab_size, input_size, hidden_dim, output_size):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(src_vocab_size, input_size)\n","\n","        self.W_f = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_f = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_i = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_i = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_c = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_t = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_t = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_t = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.V = nn.Linear(hidden_dim, output_size, bias=False)\n","        self.b_o = nn.Parameter(torch.zeros(output_size))\n","\n","        self.tanh = nn.Tanh()\n","        self.sigm = nn.Sigmoid()\n","        \n","\n","    def forward(self, texts, lengths, **batch):\n","        h0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","        c0 = torch.zeros(texts.shape[0], self.hidden_dim).to(device)\n","\n","        embedded_text = self.embedding(texts)\n","        all_outputs = []\n","\n","        for i in range(texts.shape[1]):\n","            x_i = embedded_text[:, i, :].squeeze(1)\n","            ft = self.sigm(self.W_f(x_i) + self.U_f(h0) + self.b_f)\n","            it = self.sigm(self.W_i(x_i) + self.U_i(h0) + self.b_i)\n","            wave_ct = self.tanh(self.W_c(x_i) + self.U_c(h0) + self.b_c)\n","            c0 = ft * c0 + it * wave_ct\n","            ot = self.sigm(self.W_t(x_i) + self.U_t(h0) + self.b_t)\n","            h0 = ot * self.tanh(c0)\n","\n","            all_outputs.append(self.V(h0) + self.b_o)\n","        \n","        last_outputs = []\n","        for i, length in enumerate(lengths):\n","            last_outputs.append(all_outputs[length - 1][i, :])\n","\n","        return torch.stack(last_outputs)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with the number of parameters.\n","        \"\"\"\n","        all_parameters = sum([p.numel() for p in self.parameters()])\n","        trainable_parameters = sum(\n","            [p.numel() for p in self.parameters() if p.requires_grad]\n","        )\n","\n","        result_info = super().__str__()\n","        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n","        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n","\n","        return result_info"]},{"cell_type":"markdown","metadata":{"id":"phQ-ka4mp0oS"},"source":["__Задание 6 (1 балл).__ В этом задании у вас есть две опции на выбор: добавить __двунаправленность__ для LSTM _или_ добавить __многослойность__. Можно сделать и то, и другое, но дополнительных баллов за это мы не дадим, только бесконечный респект. Обе модификации реализуются довольно просто (буквально 4 строчки кода, если вы аккуратно реализовали модель) и дают примерно одинаковый прирост в качестве. Сделайте выводы: стоит ли увеличивать размер модели в несколько раз?"]},{"cell_type":"code","execution_count":184,"id":"60e7aa1c","metadata":{},"outputs":[],"source":["from torch import nn\n","from torch.nn import Sequential\n","\n","\n","class LSTMBlock(nn.Module):\n","    def __init__(self, input_size, hidden_dim, output_size):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.W_f = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_f = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_i = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_i = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_c = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.W_t = nn.Linear(input_size, hidden_dim, bias=False)\n","        self.U_t = nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self.b_t = nn.Parameter(torch.zeros(hidden_dim))\n","\n","        self.V = nn.Linear(hidden_dim, output_size, bias=False)\n","        self.b_o = nn.Parameter(torch.zeros(output_size))\n","\n","        self.tanh = nn.Tanh()\n","        self.sigm = nn.Sigmoid()\n","        \n","\n","    def forward(self, x, lengths, **batch):\n","        # x shape is <bs, length, input_dim>\n","        h0 = torch.zeros(x.shape[0], self.hidden_dim).to(device)\n","        c0 = torch.zeros(x.shape[0], self.hidden_dim).to(device)\n","\n","        all_outputs = []\n","\n","        for i in range(x.shape[1]):\n","            x_i = x[:, i, :].squeeze(1)\n","            ft = self.sigm(self.W_f(x_i) + self.U_f(h0) + self.b_f)\n","            it = self.sigm(self.W_i(x_i) + self.U_i(h0) + self.b_i)\n","            wave_ct = self.tanh(self.W_c(x_i) + self.U_c(h0) + self.b_c)\n","            c0 = ft * c0 + it * wave_ct\n","            ot = self.sigm(self.W_t(x_i) + self.U_t(h0) + self.b_t)\n","            h0 = ot * self.tanh(c0)\n","\n","            all_outputs.append(self.V(h0) + self.b_o)\n","        \n","        last_outputs = []\n","        for i, length in enumerate(lengths):\n","            last_outputs.append(all_outputs[length - 1][i, :])\n","\n","        return torch.stack(last_outputs), torch.stack(all_outputs).transpose(1, 0)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with the number of parameters.\n","        \"\"\"\n","        all_parameters = sum([p.numel() for p in self.parameters()])\n","        trainable_parameters = sum(\n","            [p.numel() for p in self.parameters() if p.requires_grad]\n","        )\n","\n","        result_info = super().__str__()\n","        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n","        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n","\n","        return result_info"]},{"cell_type":"code","execution_count":185,"id":"97ca1c1e","metadata":{},"outputs":[],"source":["class LSTMMultilayer(nn.Module):\n","\n","    def __init__(self, src_vocab_size, input_size, hidden_dim, output_size, num_layers=2):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.lstms = nn.ModuleList()\n","        self.embedding = nn.Embedding(src_vocab_size, input_size)\n","        self.lstms.append(LSTMBlock(input_size, hidden_dim, hidden_dim if num_layers > 1 else output_size).to(device))\n","        for i in range(self.num_layers - 1):\n","            self.lstms.append(LSTMBlock(hidden_dim, hidden_dim, output_size if i == self.num_layers - 2 else hidden_dim).to(device))\n","        \n","    def forward(self, x, lengths):\n","        x = self.embedding(x) \n","        for lstm in self.lstms:\n","            output, x = lstm(x, lengths)\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"8bc444e8","metadata":{},"outputs":[],"source":["wandb.init(\n","    project=\"nlp_hw2_rnn\",\n","    name='testing_LSTMMultilayer',\n","\n","    config={\n","    \"max_lr\": 5e-3,\n","    \"input_size\": 128,\n","    \"hidden_dim\": 256,\n","    \"epochs\": 30,\n","    }\n",")\n","\n","train_and_validate(wandb, len(dictionary.token2id), train_dataloader, test_dataloader, LSTMMultilayer)"]},{"cell_type":"code","execution_count":172,"id":"da4a76d6","metadata":{},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"id":"f66f523d","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"widgets":{"application/vnd.jupyter.widget-state+json":{"12b0627d4aaf46c0adc64b442bf88d0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d5b2e090c51406e953b4eec4b0b91ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"282f83858a424e2ea76990eb957dc5a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32808478ae8c4242beb79f0272ea6b1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34e8d1401c0e4dc1a8e71bbad7c2f74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23f3b8b7247491c8d5e3ead7f54d886","placeholder":"​","style":"IPY_MODEL_cb632291897f4f9db86a00a5a71ca35f","value":" 40/40 [36:41&lt;00:00, 51.61s/it]"}},"3735627f227d4b4f927955113111409f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47f4f11bc6984b96ac3c3875d733f0ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4f687f9d5940aba074e2bb41581c93","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e10fd6d1a6c47a9ac34a47ae5ba708b","value":40}},"4aab16bb20824688aadbd23460adad9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f65eec1b45de42e59fb9e24b99aad917","IPY_MODEL_47f4f11bc6984b96ac3c3875d733f0ba","IPY_MODEL_f58fddb1bf414071b0523701a619ad71"],"layout":"IPY_MODEL_32808478ae8c4242beb79f0272ea6b1f"}},"4de9492961d841aa9f3d7bc629911296":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ae0c089c4a426db3b52976fae1a9dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e10fd6d1a6c47a9ac34a47ae5ba708b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b23f3b8b7247491c8d5e3ead7f54d886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4165ff8fc3480fb1590b6ecd39fb4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cba16e32a9df4b1b89b4f7066945fc42","IPY_MODEL_e8f0522f19c44066b5a78ded999f050a","IPY_MODEL_34e8d1401c0e4dc1a8e71bbad7c2f74d"],"layout":"IPY_MODEL_282f83858a424e2ea76990eb957dc5a0"}},"cb632291897f4f9db86a00a5a71ca35f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cba16e32a9df4b1b89b4f7066945fc42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ae0c089c4a426db3b52976fae1a9dc","placeholder":"​","style":"IPY_MODEL_12b0627d4aaf46c0adc64b442bf88d0a","value":"100%"}},"d7ed88f49793494bbdb3c2fffc01b216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4f687f9d5940aba074e2bb41581c93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7876fd73da349ea873c137c63d8d528":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8f0522f19c44066b5a78ded999f050a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4de9492961d841aa9f3d7bc629911296","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7876fd73da349ea873c137c63d8d528","value":40}},"f58fddb1bf414071b0523701a619ad71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d5b2e090c51406e953b4eec4b0b91ad","placeholder":"​","style":"IPY_MODEL_3735627f227d4b4f927955113111409f","value":" 40/40 [1:08:10&lt;00:00, 102.17s/it]"}},"f65eec1b45de42e59fb9e24b99aad917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f67dc08a01ac40ad98ed553fe6b7e948","placeholder":"​","style":"IPY_MODEL_d7ed88f49793494bbdb3c2fffc01b216","value":"100%"}},"f67dc08a01ac40ad98ed553fe6b7e948":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":5}
